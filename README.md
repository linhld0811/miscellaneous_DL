# miscellaneous_DL
1. the normalization layer (be it BatchNorm, LayerNorm or whatever) after the convolutional layer and before the activation layer, i.e. Conv + Norm + ReLU.

# To read
1. [Accurately Time CUDA Kernels in Pytorch](https://www.speechmatics.com/company/articles-and-news/timing-operations-in-pytorch)
2. [torch_tensorrt](https://pytorch.org/TensorRT/getting_started/getting_started_with_python_api.html#getting-started-with-python-api)
[tutor](https://pytorch.org/TensorRT/user_guide/runtime.html), [tutor](https://pytorch.org/TensorRT/tutorials/use_from_pytorch.html)
3. [torch compiler](https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler-overview)
4. [intel extention for pytorch](https://intel.github.io/intel-extension-for-pytorch/#introduction)
5. 
